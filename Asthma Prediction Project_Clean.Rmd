---
title: "Asthma Attack Predictions"
author: "Kadie Iverson & Matthew Donaldson"
date: "2/8/2022"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

---
title: "Asthma Attack Predictions"
author: "Kadie Iverson & Matthew Donaldson"
date: "2/8/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(car)
library(olsrr)
library(lme4)
library(shiny)
library(cvms)
library(groupdata2)
library(lmerTest)
library(glmnet)
library(glmmLasso)
library(boot)
library(arm)

#install.packages(c("boot", "car", "caret", "tidyverse",  "effects", "foreign", 
                   #"Hmisc", "DT", "knitr", "lme4", "MASS", "mlogit", "msm", 
                   #"QuantPsyc", "reshape2", "rms", "sandwich", "sfsmisc", "sjPlot", 
                   #"vcd", "visreg", "MuMIn", "lmerTest", "shiny"))
```

--\> how each subject rates their asthma --\> look at the type of
factoring (maybe 0,1,2) --\> standardized after split into training

lmer: reaction \~ days + (days\|subject)

generalized linear model - randomness within the variance of the error
term

TO DO:

-   In lmerTest package function to use: step.lmerNodLmerTest() -\>
    backward elimination of Rand/fixed effects

-   Also ANOVA() tests rand effects and drop1() tests fixed effects
    (f-test).

-   Read panel data, (mixture model source)

-   make a decision on whether to have data from each participant or
    hold out on 2 participants

Sources: - Mixture model:
<https://lme4.r-forge.r-project.org/slides/2011-03-16-Amsterdam/2Longitudinal.pdf> -
Catet Package:
<https://topepo.github.io/caret/feature-selection-overview.html> -
Vraible selection:
<https://cran.r-project.org/web/packages/olsrr/vignettes/variable_selection.html> -
Boxplots: <http://www.unige.ch/ses/sococ/cl/r/bapr.e.html> - Regressing
Cat. data:
<https://advstats.psychstat.org/book/mregression/catpredictor.php> -
Coding Cat. Data:
<https://stats.oarc.ucla.edu/r/modules/coding-for-categorical-variables-in-regression-models/>

# Practicum of Data Analytics Project

## Importing

```{r}
################################################################################
# Importing the data set 
asthmaDataOriginal= read.csv("Asthma_Data_File.csv")  

# Removing id number for each subject and copying original file to not corrupt it
asthmaData = data.frame(asthmaDataOriginal[,-2])  

# Looking at data
head(asthmaData)  #Looking at the first 6 entries
dim(asthmaData)
```

## Data Description

The first summary doesn't give us to much information because half the
inputs are chr. So, before looking at the properties of each feature we
need to factor the data.

```{r}
# Summary gives a description for each input and output, giving you a sense of what the data looks like
summary(asthmaData)

```

Factoring is done in two stages. One for variables were there is an order and 
one for no order. For no order the lapply() function is used and for when we want
to specify order the factor() function with order = TRUE is used. The we check using
the str() function.

```{r}
# Only want  to factor the categories.
# Ordered categorical variables: Age, OutDoorJob, OutDoorActivities, UVIndex
# Else: just categorical variables to factor.
# Continuous Varaibale: temp, humidity, windspeed and pressure
# Dependent variable: ACTScore
names = names(asthmaData[,c(1,2, 4,7)]) 
asthmaData[,names] = lapply(asthmaData[,names], factor)

asthmaData[,3] = factor(asthmaData[,3], order = TRUE, levels = c("19-30", "31-40", "41-50", "Above 50"))
asthmaData[,5] = factor(asthmaData[,5], order = TRUE, levels = c("Rarely", "Occasionally", "Frequently"))
asthmaData[,6] = factor(asthmaData[,6], order = TRUE, levels = c("Not at all likely", "Neither likely or dislikely", "Extremely likely"))
asthmaData[,11] = factor(asthmaData[,11], order = TRUE, levels = c("Low", "Extreme"))

str(asthmaData)

```

Next we aim to see the distribution for each variable and subjects by bar plots
and histograms. As can be seen from the plots below, we have unbalanced instances
for subjects, outdoor activities and jobs...


```{r}
barplot(prop.table(table(asthmaData$UserNo.)), main = 'Subject')
barplot(prop.table(table(asthmaData$Age)), main = 'Age')
barplot(prop.table(table(asthmaData$Gender)), main = 'Gender')
barplot(prop.table(table(asthmaData$OutdoorJob)), main = 'Out Door Job')
barplot(prop.table(table(asthmaData$OutdoorActivities)), main = 'Out Door Activities')
barplot(prop.table(table(asthmaData$SmokingHabit)), main = 'Smoking Habit')
barplot(prop.table(table(asthmaData$UVIndex)), main = 'UV Index')

hist(asthmaData$Humidity, breaks = 25, main = 'Humidity')
hist(asthmaData$Temperature, breaks = 25, main = 'temp')
hist(asthmaData$WindSpeed, breaks = 25, main = 'wind')
hist(asthmaData$Pressure, breaks = 12, main = 'Pressure')
hist(asthmaData$ACTScore,breaks = 17, main = 'ACT score')
```

This is just to get the actual values for each level in the groups based on the 
bar and histograms above.
```{r}
table(asthmaData$Age) 
table(asthmaData$Gender)
table(asthmaData$OutdoorJob)
table(asthmaData$OutdoorActivities)
table(asthmaData$SmokingHabit)
table(asthmaData$UVIndex)
table(asthmaData$ACTScore)
################################################################################
```




## Splitting into Train & Test

Before anything else, we split the data into a training and test set. This ensures
that we do not have any data leakage when we standardize our training set and so
the test set is left alone.
```{r}
################################################################################
set.seed(24)

library(dplyr)
training <- sample(unique(asthmaData$UserNo.), 7, replace = FALSE)
training

train <- asthmaData %>% filter(asthmaData$UserNo. %in% training)
test <-  asthmaData %>% filter(!asthmaData$UserNo. %in% training)

# This is only used to ensure that once we remove the outliers we done continue
# to remove more indices if we run more than once. (Forshadowing to fix a problem)
train_original = train

# Looking at dimensions of training and test set
dim(train)
dim(test)
################################################################################
```


## Outlier Detection



To find outliers, we will have a box plot of each input that is
categorical and put it against the inputs that are continuous.Finding
the outliers using Boxplot (finds points outside 1.5\*IQR), as well as
to get a better understanding the distribution of the features a box
plot was made. We do this for the training set.

First plotting humidity against the categorical variables

```{r}
###############################################################################
## NEED TO USE PACKAGE car for this to work because of the upper case B

# This box and whisker plot created the boxplot and also labels the points that are 
# 1.5 times outside the IQR, I then take those points, convert them from chr to
# numberic and store it as an outlier. I think take the union of all the outlires
# to keep track of when looking at other inputs that are continuous.

# Source for this: http://www.unige.ch/ses/sococ/cl/r/bapr.e.html

outlier_ageH = as.numeric(Boxplot(asthmaData$Humidity~asthmaData$Age))
outlier_genderH = as.numeric(Boxplot(asthmaData$Humidity~asthmaData$Gender))
outlier_smH = as.numeric(Boxplot(asthmaData$Humidity~asthmaData$SmokingHabit))
outlier_odjH = as.numeric(Boxplot(asthmaData$Humidity~asthmaData$OutdoorJob))
outlier_odaH = as.numeric(Boxplot(asthmaData$Humidity~asthmaData$OutdoorActivities))
outlier_uvH = as.numeric(Boxplot(asthmaData$Humidity~asthmaData$UVIndex))

# The last part combines all the outlier indices found as a vector
totOutlier_H = c(outlier_ageH, outlier_genderH, outlier_smH, outlier_odjH, outlier_odaH, outlier_uvH)
table(totOutlier_H) # Potential outlier indecies
```

Next plotting temp. against cat.

```{r}
## NEED TO USE PACKAGE car for this to work because of the upper case B

# This box and whisker plot created the boxplot and also labels the points that are 
# 1.5 times outside the IQR, I then take those points, convert them from chr to
# numberic and store it as an outlier. I think take the union of all the outliers
# to keep track of when looking at other inputs that are continuous.

outlier_ageT = as.numeric(Boxplot(asthmaData$Temperature~asthmaData$Age))
outlier_genderT = as.numeric(Boxplot(asthmaData$Temperature~asthmaData$Gender))
outlier_smT = as.numeric(Boxplot(asthmaData$Temperature~asthmaData$SmokingHabit))
outlier_odjT = as.numeric(Boxplot(asthmaData$Temperature~asthmaData$OutdoorJob))
outlier_odaT = as.numeric(Boxplot(asthmaData$Temperature~asthmaData$OutdoorActivities))
outlier_pT = as.numeric(Boxplot(asthmaData$Temperature~asthmaData$Pressure))
outlier_uvT = as.numeric(Boxplot(asthmaData$Temperature~asthmaData$UVIndex))

# The last part combines all the outlier indices found as a vector and keeps all 
# number once, so say 114 is stored twice then the unique() will just take one 
# of them out
totOutlier_T= c(outlier_ageT, outlier_genderT, outlier_smT, outlier_odjT, outlier_odaT,outlier_pT, outlier_uvT)

table(totOutlier_T)
```

plotting windspeed with cat.

```{r}
# This box and whisker plot created the boxplot and also labels the points that are 
# 1.5 times outside the IQR, I then take those points, convert them from chr to
# numberic and store it as an outlier. I think take the union of all the outliers
# to keep track of when looking at other inputs that are continuous.

outlier_ageW = as.numeric(Boxplot(asthmaData$WindSpeed~asthmaData$Age))
outlier_genderW = as.numeric(Boxplot(asthmaData$WindSpeed~asthmaData$Gender))
outlier_smW = as.numeric(Boxplot(asthmaData$WindSpeed~asthmaData$SmokingHabit))
outlier_odjW = as.numeric(Boxplot(asthmaData$WindSpeed~asthmaData$OutdoorJob))
outlier_odaW = as.numeric(Boxplot(asthmaData$WindSpeed~asthmaData$OutdoorActivities))
outlier_uvW = as.numeric(Boxplot(asthmaData$WindSpeed~asthmaData$UVIndex))

# The last part combines all the outlier indices found as a vector and keeps all 
# number once, so say 114 is stored twice then the unique() will just take one 
# of them out

totOutlier_W = c(outlier_ageW, outlier_genderW, outlier_smW, outlier_odjW, outlier_odaW, outlier_uvW)
table(totOutlier_W)

```

Plotting pressure with cat.
```{r}
# This box and whisker plot created the boxplot and also labels the points that are 
# 1.5 times outside the IQR, I then take those points, convert them from chr to
# numberic and store it as an outlier. I think take the union of all the outliers
# to keep track of when looking at other inputs that are continuous.

outlier_ageP = as.numeric(Boxplot(asthmaData$Pressure~asthmaData$Age))
outlier_genderP = as.numeric(Boxplot(asthmaData$Pressure~asthmaData$Gender))
outlier_smP = as.numeric(Boxplot(asthmaData$Pressure~asthmaData$SmokingHabit))
outlier_odjP = as.numeric(Boxplot(asthmaData$Pressure~asthmaData$OutdoorJob))
outlier_odaP = as.numeric(Boxplot(asthmaData$Pressure~asthmaData$OutdoorActivities))
outlier_uvP = as.numeric(Boxplot(asthmaData$Pressure~asthmaData$UVIndex))

# The last part combines all the outlier indices found as a vector and keeps all 
# number once, so say 114 is stored twice then the unique() will just take one 
# of them out

totOutlier_P = c(outlier_ageP, outlier_genderP, outlier_smP, outlier_odjP, outlier_uvP)
table(totOutlier_P)

```


Now combining all indices found to be potential outliers and taking any indices 
that was found more than once.
```{r}
# Combines all outliers into on vector.
totOutliers = c(totOutlier_H, totOutlier_T, totOutlier_W, totOutlier_P)

#Gives the indices for the the ones that show up more than once. 
repOutliers = which(table(totOutliers) > 1) 

# This ensures that if we run the code again it will not take more
# Data out, just noticed it was going down when this was not added

# Here we use the train_original from when we made the test set to compare if 
# outliers have already been taken out.
if(dim(train)[1] == dim(train_original)[1]){
  train= train[-c(repOutliers),]
}
# Visual help that outliers are only removed once
dim(asthmaData)
dim(train)
length(repOutliers)
################################################################################
```


## Standardize Data set

```{r}
################################################################################
## standard normalize of continuous variable for mixture model.
train$Humidity = scale(train$Humidity, scale = TRUE)
train$Temperature = scale(train$Temperature, scale = TRUE)
train$WindSpeed = scale(train$WindSpeed, scale = TRUE)
train$Pressure = scale(train$Pressure, scale = TRUE)

test$Humidity = scale(test$Humidity, scale = TRUE)
test$Temperature = scale(test$Temperature, scale = TRUE)
test$WindSpeed = scale(test$WindSpeed, scale = TRUE)
test$Pressure = scale(test$Pressure, scale = TRUE)
################################################################################
```


## Understanding Correlation b/w variables through VIF number

We try and understand the correlation of a variable through the VIF number.
VIF provides an index that measure show much the variance of an estimated 
regression coefficient is increased because of linearity. If VIF is above
5 or 10 then it is an indication that the associated regression coefficients are 
poorly estimated due to multicollinearity.
```{r}
library(faraway)
model.vif = lm(ACTScore ~ Temperature + WindSpeed + Age + SmokingHabit + Humidity + Gender + OutdoorJob + OutdoorActivities + Pressure + UVIndex, data = train)

vif(model.vif)


```

## Feature Selection for mixed effects model

We first perform backwards feature selection with our mixed effects model. In the
step() function from lmerTest package, the function first looks at the significance of
random effects then looks at the fixed effects. Picks values to drop based on 
p-values. 
```{r}
################################################################################

# https://www.rdocumentation.org/packages/lmerTest/versions/2.0-36/topics/step

model.lmer.step <- lmerTest::lmer(ACTScore ~ Temperature + WindSpeed + Age + SmokingHabit + Humidity + Gender + OutdoorJob + OutdoorActivities + Pressure + UVIndex + (1|UserNo.) + (1|Location) + (0 + Gender|UserNo.) + (0 + Gender|Location) + (0 + Age|UserNo.) + (0 + Age|Location), data = train)

step_res.lmer <- lmerTest::step(model.lmer.step)
model.lmer <- get_model(step_res.lmer)
anova(model.lmer)
step_res.lmer


```
From the output above we can see the model found is:
ACTScore ~ WindSpeed + Humidity + Gender + OutdoorJob + Pressure + UVIndex

From this first we notice that none of the random effects are used. 
And then also 6 varaibles are left.


## Feature Selection for fixed linear model

Next we do feature selection on a fixed linear model. This function chooses the 
features to from by AIC (), but other wise same process as step() function above.
```{r}
model.lm <- lm(ACTScore ~ Temperature + WindSpeed + Age + SmokingHabit + Humidity + Gender + OutdoorJob + OutdoorActivities + Pressure + UVIndex, data = train)

step_res.lm <- step(model.lm)

###############################################################################
```

model found from above:
ACTScore ~ WindSpeed + Humidity + Gender + OutdoorJob + Pressure + UVIndex
Notice this is the same as if we first started out with random effect!!!

```{r}
model.lm = lm(ACTScore ~ WindSpeed + Humidity + Gender + OutdoorJob + Pressure + UVIndex, data = train)
################################################################################
```

## Bootstrapping C.I. for parameters



## Predict on Test Set













```{r}
################# 
# just trying to write own boot strap, honestly got fed up with finding a boot
# strap function that would work on the different models.
library(resample)
set.seed(18)
n = 1
sample_index = samp.bootstrap(n = dim(train)[1], R = n, size = dim(train)[1], reduceSize = 0)

for (i in 1:n)  {
  # taking sampled indices and making the data set to put into model
  # as well as the out of bag (OOB) indices to "validate".
  train_boot = train[sample_index[,i],]
  OOB  = train[-sample_index[,i],]
  
  
  
  # Training different models and predicting on them with the out of bag values
# model1 = lmerTest::lmer(ACTScore ~ WindSpeed  + Humidity + Gender + OutdoorJob  + Pressure + UVIndex + (1|UserNo.) + (1|Location) + (0 + Gender|UserNo.) + (0 + Gender|Location) + (0 + Age|UserNo.) + (0 + Age|Location), data = train_boot)
  
    
  model3 = lm(ACTScore ~ WindSpeed + Age + SmokingHabit + Humidity + OutdoorJob + 
   Pressure + UVIndex, data = train_boot)
  

   pred3 = predict(model3, OOB, interval="predict") 
   
   
  #x = data.matrix(train_boot[,c('Humidity', 'Temperature', 'Pressure', 'Age', 'Gender', 'UVIndex', 'OutdoorJob', 'OutdoorActivities', 'SmokingHabit', 'WindSpeed')])
 # y = train_boot[,'ACTScore']

  #model4 = glmnet(x,y, alpha = cv.lasso$lambda.1se)
  
  # Predicting on OOB samles based on trained set
  #pred1 = predict(model1, OOB, interval="predict") 
  
 
  
  #newData = as.data.frame(OOB[,c('Humidity', 'Temperature', 'Pressure', 'Age', 'Gender', 'UVIndex', 'OutdoorJob', 'OutdoorActivities', 'SmokingHabit', 'WindSpeed')])
  #pred4 = predict(model4, newData, interval="predict") 
  
  
  
}

#unique(train$Pressure)
#unique(train_boot$Pressure)
#unique(OOB$Pressure)
```



## Models

### Backward feature selection




```{r}
########################################################################
## This gives a boot strap of the r squared 

model.lmer = lmerTest::lmer(ACTScore ~ WindSpeed + Age + SmokingHabit + Humidity + Gender + OutdoorJob + OutdoorActivities + Pressure + UVIndex + (1|UserNo.) + (1|Location) + (0 + Gender|UserNo.) + (0 + Gender|Location) + (0 + Age|UserNo.) + (0 + Age|Location), data = train)

rsq <- function(formula) {
  fit <- lm(formula)
  return(summary(fit)$r.squared)
}

boot.stats = bootMer(model.lmer, rsq, nsim = 10, seed = 46, use.u = FALSE, re.form=NA,
	type = c("parametric", "semiparametric"),
	verbose = FALSE, .progress = "none", PBargs = list(),
	parallel = c("no", "multicore", "snow"),
	ncpus = getOption("boot.ncpus", 1L), cl = NULL)

```

```{r}
# Looking at value of r2
boot.stats$t
boot.stats$t0
boot::boot.ci(boot.stats, index=1, type=c("norm", "basic", "perc")) #Gives C.I.

```

```{r}

## This gives a boot strap of the MSE 
mse <- function(formula) {
  fit <- lmer(formula)
  return(summary(fit)$r.squared)
}

boot.stats = bootMer(model.lmer, mse, nsim = 4, seed = 46, use.u = FALSE, re.form=NA,
	type = c("parametric", "semiparametric"),
	verbose = FALSE, .progress = "none", PBargs = list(),
	parallel = c("no", "multicore", "snow"),
	ncpus = getOption("boot.ncpus", 1L), cl = NULL)
#############################################################################
```

## Testing lasso with model

```{r}
#set lambdas... go from 0 to 10^5, in 10 log steps
lambda <- seq(0.001,1, length = 20)
BIC_vec <- rep(Inf, length(lambda))
AIC_vec <- rep(Inf, length(lambda))


# The for loop tests the different lambda values and records the BIC number.
# This is hypertuning the parameter lambda through
for (j in 1:20)
  {
    print(paste("Iteration ", j, sep=""))

glm1 =  try(glmmLasso(ACTScore~ WindSpeed + Temperature + as.factor(Gender) + as.factor(Pressure) + as.factor(UVIndex) + as.factor(Age)  + as.factor(OutdoorJob) +  Humidity,rnd = list(UserNo.=~1,Location=~ 1), lambda=lambda[j], data = train, switch.NR = TRUE,final.re = TRUE))

# Save BIC number 
if(class(glm1)!="try-error")
      {  
      BIC_vec[j]<-glm1$bic
      AIC_vec[j] = glm1$aic
    
      }
  }

```



```{r}
plot(BIC_vec)
plot(AIC_vec)

which.min(BIC_vec)
lambda[which.min(BIC_vec)]

```

```{r}
model2 = glmmLasso(ACTScore~ WindSpeed + Humidity + Temperature + as.factor(Gender) + as.factor(OutdoorJob) + as.factor(Pressure) + as.factor(Age) + as.factor(UVIndex),rnd = list(UserNo.=~1 ,Location=~ 1), lambda = 0.1061579, data = train, switch.NR = TRUE,final.re = TRUE)

# https://www.aggieerin.com/post/lasso-myself-numbers/
#0.4132323

summary(model2)
```


```{r}
#model_l$bic
#model_l$aic
model2$ranef

```

It has been confirmed, by backwards feature selection and Lasso that the random effects terms are not significant. This can be seen from the fact that backwards feature method did not select the random effects and the lasso method found the coefficients to be nearly zero.

So we try fitting with a linear model.



```{r}
library(emdi)
n = dim(train)[1]
backmodel2 <- lm(ACTScore ~ WindSpeed + Age + SmokingHabit + Humidity + Gender + OutdoorJob + OutdoorActivities + Pressure + UVIndex, data = train)

step_res2 <- step(backmodel2, criteria = "BIC")
#final2 <- get_model(step_res2)
#anova(final2)
#step_res2$model


summary(step_res2)
step_res2$anova

model3 = lm(ACTScore ~ WindSpeed + Age + SmokingHabit + Humidity + OutdoorJob + 
    Pressure + UVIndex, data = train)


```


## LASSO with out random effects

```{r}

'%ni%' = Negate('%in%')
x = data.matrix(train[,c('Humidity', 'Temperature', 'Pressure', 'Age', 'Gender', 'UVIndex', 'OutdoorJob', 'OutdoorActivities', 'SmokingHabit', 'WindSpeed')])
y = train[,'ACTScore']
cv.lasso = cv.glmnet(x,y,alpha=1)
cv.lasso$lambda.min # optimal penalty parameter
cv.lasso$lambda.1se
plot(cv.lasso)

c = coef(cv.lasso, s = 'lambda.min', exact = TRUE)
inds = which(c!=0)
variables  = row.names(c)[inds]
variables = variables[variables %ni% '(Intercept)']
variables

model4 = glmnet(x,y, alpha = cv.lasso$lambda.1se)


```



## Bootstrap lm function

```{r}


rsq <- function(formula, data, indices) {
  d <- data[indices,]
  fit <- lm(formula, data=d)
  return(summary(fit)$r.squared)
}


rs <- vector()
j <- 1

results3 <- boot(data = train, statistic = rsq, R = 10, formula = model3)

  
boot.ci(results3,type="perc",index=1)
results3$t0


```

```{r}


mse <- function(formula, data, indices) {
  d <- data[indices,]
  fit <- lm(formula, data=d)
  return(sum(fit$residuals^2))
}


rs <- vector()
j <- 1

results3 <- boot(data = train, statistic = mse, R = 10, formula = model3)

  
boot.ci(results3,type="perc",index=1)
results3$t0
###############################################################################

```


# Predicting on validation set
```{r}
pred3 = predict(model3, test, interval="predict") 
error3 = round(pred3) - test$ACTScore

#pred_sb = predict(model_sb, validation, interval='predict')
#error_sb =  round(pred_sb[,1]) - validation$ACTScore
#error_sb

data.frame(R2 = R2(pred3, test $ ACTScore),
           RMSE = RMSE(pred3, test $ ACTScore), 
           MAE = MAE(pred3, test $ ACTScore))

#data.frame(R2 = R2(pred_sb, validation $ ACTScore), 
#           RMSE = RMSE(pred_sb, validation $ ACTScore), 
#           MAE = MAE(pred_sb, validation $ ACTScore))
```


# Generaliation to test set
```{r}

BestModel = predict(model_l, test, interval="predict") 
error_l = round(BestModel) - test$ACTScore

data.frame(R2 = R2(BestModel, test $ ACTScore),
           RMSE = RMSE(BestModel, test $ ACTScore), 
           MAE = MAE(BestModel, test $ ACTScore))
```













