---
title: "Asthma Attack Predictions"
author: "Kadie Iverson & Matthew Donaldson"
date: "2/8/2022"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

---
title: "Asthma Attack Predictions"
author: "Kadie Iverson & Matthew Donaldson"
date: "2/8/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(car)
library(olsrr)
library(lme4)
library(shiny)
library(cvms)
library(groupdata2)
library(lmerTest)
library(glmnet)
library(glmmLasso)
library(boot)
library(arm)

#install.packages(c("boot", "car", "caret", "tidyverse",  "effects", "foreign", 
                   #"Hmisc", "DT", "knitr", "lme4", "MASS", "mlogit", "msm", 
                   #"QuantPsyc", "reshape2", "rms", "sandwich", "sfsmisc", "sjPlot", 
                   #"vcd", "visreg", "MuMIn", "lmerTest", "shiny"))
```

--\> how each subject rates their asthma --\> look at the type of
factoring (maybe 0,1,2) --\> standardized after split into training

lmer: reaction \~ days + (days\|subject)

generalized linear model - randomness within the variance of the error
term

TO DO:

-   In lmerTest package function to use: step.lmerNodLmerTest() -\>
    backward elimination of Rand/fixed effects

-   Also ANOVA() tests rand effects and drop1() tests fixed effects
    (f-test).

-   Read panel data, (mixture model source)

-   make a decision on whether to have data from each participant or
    hold out on 2 participants

Sources: - Mixture model:
<https://lme4.r-forge.r-project.org/slides/2011-03-16-Amsterdam/2Longitudinal.pdf> -
Catet Package:
<https://topepo.github.io/caret/feature-selection-overview.html> -
Vraible selection:
<https://cran.r-project.org/web/packages/olsrr/vignettes/variable_selection.html> -
Boxplots: <http://www.unige.ch/ses/sococ/cl/r/bapr.e.html> - Regressing
Cat. data:
<https://advstats.psychstat.org/book/mregression/catpredictor.php> -
Coding Cat. Data:
<https://stats.oarc.ucla.edu/r/modules/coding-for-categorical-variables-in-regression-models/>

# Practicum of Data Analytics Project

## Importing

```{r}
asthmaDataOriginal= read.csv("Asthma_Data_File.csv")  # Importing the data set 

asthmaData = data.frame(asthmaDataOriginal[,-2])  # Removing id number for each subject

head(asthmaData)  #Looking at the first 6 entries
dim(asthmaData)
```

### Data Description

The first summary doesn't give us to much information because half the
inputs are chr. So, before looking at the properties of each feature we
need to convert the strings to integers.

```{r}
# Summary gives a description for each input and output, giving you a sense of what the data looks like
summary(asthmaData)
```

Removing the subject number and all the non categorical data. Then
factoring each input. Now there are levels for each categorical input.

```{r}
# Only want  to factor the categories so removing the columns listed below.
# These are: temp, humidity, windspeed and ACTScore.
names = names(asthmaData[,c(1,2, 4,7)]) 
asthmaData[,names] = lapply(asthmaData[,names], factor)

asthmaData[,3] = factor(asthmaData[,3], order = TRUE, levels = c("19-30", "31-40", "41-50", "Above 50"))
asthmaData[,5] = factor(asthmaData[,5], order = TRUE, levels = c("Rarely", "Occasionally", "Frequently"))
asthmaData[,6] = factor(asthmaData[,6], order = TRUE, levels = c("Not at all likely", "Neither likely or dislikely", "Extremely likely"))
asthmaData[,9] = factor(asthmaData[,9], order = TRUE, levels = c("1003", "1004", "1005", "1006", "1007", "1008", "1009", "1010", "1011", "1012", "1013", "1014"))
asthmaData[,11] = factor(asthmaData[,11], order = TRUE, levels = c("Low", "Extreme"))

str(asthmaData)

```





Bar plots and histograms give us an idea of the data in each category.

```{r}
barplot(prop.table(table(asthmaData$UserNo.)), main = 'Subject')
barplot(prop.table(table(asthmaData$Age)), main = 'Age')
barplot(prop.table(table(asthmaData$Gender)), main = 'Gender')
barplot(prop.table(table(asthmaData$OutdoorJob)), main = 'Out Door Job')
barplot(prop.table(table(asthmaData$OutdoorActivities)), main = 'Out Door Activities')
barplot(prop.table(table(asthmaData$SmokingHabit)), main = 'Smoking Habit')
barplot(prop.table(table(asthmaData$Pressure)), main = 'Pressure')
barplot(prop.table(table(asthmaData$UVIndex)), main = 'UV Index')

hist(asthmaData$Humidity, breaks = 25, main = 'Humidity')
hist(asthmaData$Temperature, breaks = 25, main = 'temp')
hist(asthmaData$WindSpeed, breaks = 25, main = 'wind')
hist(asthmaData$ACTScore,breaks = 17, main = 'ACT score')
```

```{r}
table(asthmaData$Age) 
table(asthmaData$Gender)
table(asthmaData$OutdoorJob)
table(asthmaData$OutdoorActivities)
table(asthmaData$SmokingHabit)
table(asthmaData$Pressure)
table(asthmaData$UVIndex)
table(asthmaData$ACTScore)
```

Looking at frequency and distribution of the ACT score that each user
self reported.

```{r}

#jpeg(file="saving_plot1.jpeg") 
#ggplot(asthmaData, aes(x = ACTScore)) +
#  geom_histogram(fill = "white", colour = "black", bins = 20) +
#  facet_grid(UserNo. ~ .)

```

## Splitting into Train & Test

```{r}
set.seed(28)

#groups <-        # Randomly assign train/test groups to all values of UserNo.
#  asthmaData %>%
#  select(asthmaData$UserNo.) %>%
#  distinct(asthmaData$UserNo.) %>%
#  rowwise() %>%
#  mutate(group = sample(
#    c("train", "test"),
#    1,
#    replace = TRUE,
#    prob = c(0.8, 0.2) 
#  ))
#groups

# Join group assignments to my_dat
#asthmaData <- asthmaData %>%
#  left_join(groups)

#validation = filter(asthmaData, group == "validation")
#test = filter(asthmaData, group == "test")

#dim(train)

#index = which(asthmaData$UserNo. == c(1,2,3,5,7,9,10))

```

```{r}
set.seed(300)
#train = subset(asthmaData, UserNo. %in% c(1,2,3,5,7,9,10))
#test = subset(asthmaData, UserNo. %in% c(4,6,8))

samp = createDataPartition(asthmaData$ACTScore, p = 0.7, list = F)

train = asthmaData[samp,]# note: it just so happens that pressure level 1003 (only one instance) is in the training.
test = asthmaData[-samp,]
#test = test[-which(test$Pressure == 1003),]

```

```{r}
# Train Set
# Catagorical Data
subject = train$UserNo.
loc = train$Location
gender = train$Gender
age = train$Age
ODJ = train$OutdoorJob
ODA = train$OutdoorActivities
smoking = train$SmokingHabit
pressure = train$Pressure
uv = train$UVIndex
# Continous Data
temp = train$Temperature
hum = train$Humidity
wind = train$WindSpeed
# Y value
ACT = train$ACTScore

# Test Set
# Catagorical Data
test_subject = test$UserNo.
test_loc = test$Location
test_gender = test$Gender
test_age = test$Age
test_ODJ = test$OutdoorJob
test_ODA = test$OutdoorActivities
test_smoking = test$SmokingHabit
test_pressure = test$Pressure
test_uv = test$UVIndex
# Continous Data
test_temp = test$Temperature
test_hum = test$Humidity
test_wind = test$WindSpeed
# Y value
test_ACT = test$ACTScore
```

## Finding outliers in the categorical/numeric data

To find outliers, we will have a box plot of each input that is
categorical and put it against the inputs that are continuous.Finding
the outliers using Boxplot (finds points outside 1.5\*IQR), as well as
to get a better understanding the distribution of the features a box
plot was made. We do this for each, train and test, set.

First plotting 1 categorical input at a time with temp humidity

```{r}
## NEED TO USE PACKAGE car for this to work because of the upper case B

# This box and whisker plot created the boxplot and also labels the points that are 
# 1.5 times outside the IQR, I then take those points, convert them from chr to
# numberic and store it as an outlier. I think take the union of all the outlires
# to keep track of when looking at other inputs that are continuous.

# Source for this: http://www.unige.ch/ses/sococ/cl/r/bapr.e.html

outlier_ageH = as.numeric(Boxplot(hum~age))
outlier_genderH = as.numeric(Boxplot(hum~gender))
outlier_smH = as.numeric(Boxplot(hum~smoking))
outlier_ODJH = as.numeric(Boxplot(hum~ODJ))
outlier_ODAH = as.numeric(Boxplot(hum~ODA))
outlier_pH = as.numeric(Boxplot(hum~pressure))
outlier_UVH = as.numeric(Boxplot(hum~uv))

# The last part combines all the outlier indices found as a vector and keeps all 
# number once, so say 114 is stored twice then the unique() will just take one 
# of them out
totOutlier_H = c(outlier_ageH, outlier_genderH, outlier_smH, outlier_ODJH, outlier_ODAH,outlier_pH, outlier_UVH)
table(totOutlier_H)
```

Next plotting 1 cat. at a time with

```{r}
## NEED TO USE PACKAGE car for this to work because of the upper case B

# This box and whisker plot created the boxplot and also labels the points that are 
# 1.5 times outside the IQR, I then take those points, convert them from chr to
# numberic and store it as an outlier. I think take the union of all the outliers
# to keep track of when looking at other inputs that are continuous.

outlier_ageT = as.numeric(Boxplot(temp~age))
outlier_genderT = as.numeric(Boxplot(temp~gender))
outlier_smT = as.numeric(Boxplot(temp~smoking))
outlier_ODJT = as.numeric(Boxplot(temp~ODJ))
outlier_ODAT = as.numeric(Boxplot(temp~ODA))
outlier_pT = as.numeric(Boxplot(temp~pressure))
outlier_UVT = as.numeric(Boxplot(temp~uv))

# The last part combines all the outlier indices found as a vector and keeps all 
# number once, so say 114 is stored twice then the unique() will just take one 
# of them out
totOutlier_T= c(outlier_ageT, outlier_genderT, outlier_smT, outlier_ODJT, outlier_ODAT,outlier_pT, outlier_UVT)

table(totOutlier_T)
```

lastly, plotting 1 cat at a time with windspeed

```{r}
# This box and whisker plot created the boxplot and also labels the points that are 
# 1.5 times outside the IQR, I then take those points, convert them from chr to
# numberic and store it as an outlier. I think take the union of all the outliers
# to keep track of when looking at other inputs that are continuous.

outlier_ageW = as.numeric(Boxplot(wind~age))
outlier_genderW = as.numeric(Boxplot(wind~gender))
outlier_smW = as.numeric(Boxplot(wind~smoking))
outlier_ODJW = as.numeric(Boxplot(wind~ODJ))
outlier_ODAW = as.numeric(Boxplot(wind~ODA))
outlier_pW = as.numeric(Boxplot(wind~pressure))
outlier_UVW = as.numeric(Boxplot(wind~uv))

# The last part combines all the outlier indices found as a vector and keeps all 
# number once, so say 114 is stored twice then the unique() will just take one 
# of them out

totOutlier_W = c(outlier_ageW, outlier_genderW, outlier_smW, outlier_ODJW, outlier_ODAW,outlier_pW, outlier_UVW)
table(totOutlier_W)


#Looking at the total amount of outliers now:


totOutliers = c(totOutlier_H, totOutlier_T, totOutlier_W)
#length(totOutliers)
#table(totOutliers)
#table(totOutliers) > 1
#which(table(totOutliers) > 1)

repOutliers = which(table(totOutliers) > 1) #Gives the indices for the the ones that show up more than once. 

# This ensures that if we run the code again it will not take more
# Data out, just noticed it was going down when this was not added
if(dim(train)[1] == 757){
  train= train[-c(repOutliers),]
}
# Visual help that outliers are only removed once
dim(asthmaData)
dim(train)
length(repOutliers)

```


## Standardize Data set

```{r}
## standard normalize of continuous variable for mixture model.
train$Humidity = scale(train$Humidity, scale = TRUE)
train$Temperature = scale(train$Temperature, scale = TRUE)
train$WindSpeed = scale(train$WindSpeed, scale = TRUE)

test$Humidity = scale(test$Humidity, scale = TRUE)
test$Temperature = scale(test$Temperature, scale = TRUE)
test$WindSpeed = scale(test$WindSpeed, scale = TRUE)


```


```{r}

# just trying to write own boot strap, honestly got fed up with finding a boot
# strap function that would work on the different models.
library(resample)
set.seed(18)

sample_index = samp.bootstrap(n = dim(train)[1], R =10, size = dim(train)[1], reduceSize = 0)

for (i in 1:10)  {
  # taking sampled indices and making the data set to put into model
  # as well as the out of bag (OOB) indices to "validate".
  train_boot = train[sample_index[,i],]
  OOB  = train[-sample_index[,i],]
  
  model3 = lm(ACTScore ~ WindSpeed + Age + SmokingHabit + Humidity + OutdoorJob + 
    Pressure + UVIndex, data = train_boot)
  
  pred3 = predict(model3, OOB, interval="predict") 
  
  
  
}



```
## Models

### Backward feature selection

```{r}


backmodel1 <- lmerTest::lmer(ACTScore ~ WindSpeed + Age + SmokingHabit + Humidity + Gender + OutdoorJob + OutdoorActivities + Pressure + UVIndex + (1|UserNo.) + (1|Location) + (0 + Gender|UserNo.) + (0 + Gender|Location) + (0 + Age|UserNo.) + (0 + Age|Location), data = train)

step_res1 <- lmerTest::step(backmodel1)
model1 <- get_model(step_res1)
anova(model1)
step_res1


```

```{r}

model1

```

## Testing lasso with model

```{r}
#set lambdas... go from 0 to 10^5, in 10 log steps
lambda <- seq(0.001,1, length = 20)
BIC_vec <- rep(Inf, length(lambda))
AIC_vec <- rep(Inf, length(lambda))


# The for loop tests the different lambda values and records the BIC number.
# This is hypertuning the parameter lambda through
for (j in 1:20)
  {
    print(paste("Iteration ", j, sep=""))

glm1 =  try(glmmLasso(ACTScore~ WindSpeed + Temperature + as.factor(Gender) + as.factor(Pressure) + as.factor(UVIndex) + as.factor(Age)  + as.factor(OutdoorJob) +  Humidity,rnd = list(UserNo.=~1,Location=~ 1), lambda=lambda[j], data = train, switch.NR = TRUE,final.re = TRUE))

# Save BIC number 
if(class(glm1)!="try-error")
      {  
      BIC_vec[j]<-glm1$bic
      AIC_vec[j] = glm1$aic
    
      }
  }

```



```{r}
plot(BIC_vec)
plot(AIC_vec)

which.min(BIC_vec)
lambda[which.min(BIC_vec)]

```

```{r}
model2 = glmmLasso(ACTScore~ WindSpeed + Humidity + Temperature + as.factor(Gender) + as.factor(OutdoorJob) + as.factor(Pressure) + as.factor(Age) + as.factor(UVIndex),rnd = list(UserNo.=~1 ,Location=~ 1), lambda = 0.1061579, data = train,switch.NR = TRUE,final.re = TRUE)

# https://www.aggieerin.com/post/lasso-myself-numbers/
#0.4132323

summary(model2)
```


```{r}
#model_l$bic
#model_l$aic
model2$ranef

```

It has been confirmed, by backwards feature selection and Lasso that the random effects terms are not significant. This can be seen from the fact that backwards feature method did not select the random effects and the lasso method found the coefficients to be nearly zero.

So we try fitting with a linear model.



```{r}
library(emdi)
n = dim(train)[1]
backmodel2 <- lm(ACTScore ~ WindSpeed + Age + SmokingHabit + Humidity + Gender + OutdoorJob + OutdoorActivities + Pressure + UVIndex, data = train)

step_res2 <- step(backmodel2, criteria = "BIC")
#final2 <- get_model(step_res2)
#anova(final2)
#step_res2$model


summary(step_res2)
step_res2$anova

model3 = lm(ACTScore ~ WindSpeed + Age + SmokingHabit + Humidity + OutdoorJob + 
    Pressure + UVIndex, data = train)
```


## LASSO with out random effects

```{r}

'%ni%' = Negate('%in%')
x = data.matrix(train[,c('Humidity', 'Temperature', 'Pressure', 'Age', 'Gender', 'UVIndex', 'OutdoorJob', 'OutdoorActivities', 'SmokingHabit', 'WindSpeed')])
y = train[,'ACTScore']
cv.lasso = cv.glmnet(x,y,alpha=1)
cv.lasso$lambda.min # optimal penalty parameter
cv.lasso$lambda.1se
plot(cv.lasso)

c = coef(cv.lasso, s = 'lambda.min', exact = TRUE)
inds = which(c!=0)
variables  = row.names(c)[inds]
variables = variables[variables %ni% '(Intercept)']
variables

model4 = glmnet(x,y, alpha = cv.lasso$lambda.1se)


```

```{r}

https://rdrr.io/github/pbreheny/hdrm/man/boot.glmnet.html
remotes::install_github("pbreheny/hdrm") # Requires the remotes package

```

```{r}
#library(pbreheny)
library(hdrm)
CI = boot.glmnet(x, y, B = 500, 0.100456, 300, alpha = 0.05, bar = TRUE)




```


```{r}

CI


```



```{r}

list = list(3,5,7,9,10)
RMSE1 = vector(length = length(list))
RMSE2 = vector(length = length(list))
RMSE3 = vector(length = length(list))
RMSE4 = vector(length = length(list))

R21 = vector(length = length(list))
R22 = vector(length = length(list))
R23 = vector(length = length(list))
R24 = vector(length = length(list))

MAE1 = vector(length = length(list))
MAE2 = vector(length = length(list))
MAE3 = vector(length = length(list))
MAE4 = vector(length = length(list))

j = 0
for (subj in list){
  
  train.cv = subset(train, UserNo. %ni% c(subj))
  validate.cv = subset(train, UserNo. %in% c(subj))
  
  model3 = lm(model3$model, data = train.cv)
 
  prediction3 = predict(model3, validate.cv, interval="predict") 
  
  #R23[j] = R2(prediction3, validate.cv $ ACTScore)
  #RMSE3[j] = RMSE(prediction3, validate.cv $ ACTScore) 
  #MAE3[j] = MAE(prediction3, validate.cv $ ACTScore)
  
  
  j = j + 1
  
  
}




```


## Boot

```{r}


rsq <- function(formula, data, indices) {
  d <- data[indices,]
  fit <- lm(formula, data=d)
  return(summary(fit)$r.squared)
}

rs <- vector()
j <- 1

  results3 <- boot(data = test, statistic = rsq, R = 1000, formula = model3)

  
boot.ci(results3,type="perc",index=1)
results3$t0
```


# Predicting on validation set
```{r}
pred3 = predict(model3, test, interval="predict") 
error3 = round(pred3) - test$ACTScore

#pred_sb = predict(model_sb, validation, interval='predict')
#error_sb =  round(pred_sb[,1]) - validation$ACTScore
#error_sb

data.frame(R2 = R2(pred3, test $ ACTScore),
           RMSE = RMSE(pred3, test $ ACTScore), 
           MAE = MAE(pred3, test $ ACTScore))

#data.frame(R2 = R2(pred_sb, validation $ ACTScore), 
#           RMSE = RMSE(pred_sb, validation $ ACTScore), 
#           MAE = MAE(pred_sb, validation $ ACTScore))
```


# Generaliation to test set
```{r}

BestModel = predict(model_l, test, interval="predict") 
error_l = round(BestModel) - test$ACTScore

data.frame(R2 = R2(BestModel, test $ ACTScore),
           RMSE = RMSE(BestModel, test $ ACTScore), 
           MAE = MAE(BestModel, test $ ACTScore))
```













