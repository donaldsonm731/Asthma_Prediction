---
title: "Asthma Attack Predictions"
author: "Kadie Iverson & Matthew Donaldson"
date: "2/8/2022"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

---
title: "Asthma Attack Predictions"
author: "Kadie Iverson & Matthew Donaldson"
date: "2/8/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(car)
library(olsrr)
library(lme4)
library(shiny)
library(cvms)
library(groupdata2)
library(lmerTest)
library(glmnet)
library(glmmLasso)
library(boot)

#install.packages(c("boot", "car", "caret", "tidyverse",  "effects", "foreign", 
                   #"Hmisc", "DT", "knitr", "lme4", "MASS", "mlogit", "msm", 
                   #"QuantPsyc", "reshape2", "rms", "sandwich", "sfsmisc", "sjPlot", 
                   #"vcd", "visreg", "MuMIn", "lmerTest", "shiny"))
```

--\> how each subject rates their asthma --\> look at the type of
factoring (maybe 0,1,2) --\> standardized after split into training

lmer: reaction \~ days + (days\|subject)

generalized linear model - randomness within the variance of the error
term

TO DO:

-   In lmerTest package function to use: step.lmerNodLmerTest() -\>
    backward elimination of Rand/fixed effects

-   Also ANOVA() tests rand effects and drop1() tests fixed effects
    (f-test).

-   Read panel data, (mixture model source)

-   make a decision on whether to have data from each participant or
    hold out on 2 participants

Sources: - Mixture model:
<https://lme4.r-forge.r-project.org/slides/2011-03-16-Amsterdam/2Longitudinal.pdf> -
Catet Package:
<https://topepo.github.io/caret/feature-selection-overview.html> -
Vraible selection:
<https://cran.r-project.org/web/packages/olsrr/vignettes/variable_selection.html> -
Boxplots: <http://www.unige.ch/ses/sococ/cl/r/bapr.e.html> - Regressing
Cat. data:
<https://advstats.psychstat.org/book/mregression/catpredictor.php> -
Coding Cat. Data:
<https://stats.oarc.ucla.edu/r/modules/coding-for-categorical-variables-in-regression-models/>

# Practicum of Data Analytics Project

## Importing

```{r}
asthmaDataOriginal= read.csv("Asthma_Data_File.csv")  # Importing the data set 

asthmaData = data.frame(asthmaDataOriginal[,-2])  # Removing id number for each subject

head(asthmaData)  #Looking at the first 6 entries
dim(asthmaData)
```

### Data Description

The first summary doesn't give us to much information because half the
inputs are chr. So, before looking at the properties of each feature we
need to convert the strings to integers.

```{r}
# Summary gives a description for each input and output, giving you a sense of what the data looks like
summary(asthmaData)
```

Removing the subject number and all the non categorical data. Then
factoring each input. Now there are levels for each categorical input.

```{r}
# Only want  to factor the categories so removing the columns listed below.
# These are: temp, humidity, windspeed and ACTScore.
names = names(asthmaData[,c(1,2, 4,7)]) 
asthmaData[,names] = lapply(asthmaData[,names], factor)

asthmaData[,3] = factor(asthmaData[,3], order = TRUE, levels = c("19-30", "31-40", "41-50", "Above 50"))
asthmaData[,5] = factor(asthmaData[,5], order = TRUE, levels = c("Rarely", "Occasionally", "Frequently"))
asthmaData[,6] = factor(asthmaData[,6], order = TRUE, levels = c("Not at all likely", "Neither likely or dislikely", "Extremely likely"))
asthmaData[,9] = factor(asthmaData[,9], order = TRUE, levels = c("1003", "1004", "1005", "1006", "1007", "1008", "1009", "1010", "1011", "1012", "1013", "1014"))
asthmaData[,11] = factor(asthmaData[,11], order = TRUE, levels = c("Low", "Extreme"))

str(asthmaData)

```





Bar plots and histograms give us an idea of the data in each category.

```{r}
barplot(prop.table(table(asthmaData$UserNo.)), main = 'Subject')
barplot(prop.table(table(asthmaData$Age)), main = 'Age')
barplot(prop.table(table(asthmaData$Gender)), main = 'Gender')
barplot(prop.table(table(asthmaData$OutdoorJob)), main = 'Out Door Job')
barplot(prop.table(table(asthmaData$OutdoorActivities)), main = 'Out Door Activities')
barplot(prop.table(table(asthmaData$SmokingHabit)), main = 'Smoking Habit')
barplot(prop.table(table(asthmaData$Pressure)), main = 'Pressure')
barplot(prop.table(table(asthmaData$UVIndex)), main = 'UV Index')

hist(asthmaData$Humidity, breaks = 25, main = 'Humidity')
hist(asthmaData$Temperature, breaks = 25, main = 'temp')
hist(asthmaData$WindSpeed, breaks = 25, main = 'wind')
hist(asthmaData$ACTScore,breaks = 17, main = 'ACT score')
```

```{r}
table(asthmaData$Age) 
table(asthmaData$Gender)
table(asthmaData$OutdoorJob)
table(asthmaData$OutdoorActivities)
table(asthmaData$SmokingHabit)
table(asthmaData$Pressure)
table(asthmaData$UVIndex)
table(asthmaData$ACTScore)
```

Looking at frequency and distribution of the ACT score that each user
self reported.

```{r}
jpeg(file="saving_plot1.jpeg") 
ggplot(asthmaData, aes(x = ACTScore)) +
  geom_histogram(fill = "white", colour = "black", bins = 20) +
  facet_grid(UserNo. ~ .)
```

## Splitting into Train & Test

```{r}
set.seed(28)

groups <-        # Randomly assign train/test groups to all values of UserNo.
  asthmaData %>%
  select(UserNo.) %>%
  distinct(UserNo.) %>%
  rowwise() %>%
  mutate(group = sample(
    c("train", "test"),
    1,
    replace = TRUE,
    prob = c(0.8, 0.2) 
  ))

groups

# Join group assignments to my_dat
asthmaData <- asthmaData %>%
  left_join(groups)

train = filter(asthmaData, group == "train")
#validation = filter(asthmaData, group == "validation")
test = filter(asthmaData, group == "test")

dim(train)
```

```{r}
# Train Set
# Catagorical Data
subject = train$UserNo.
loc = train$Location
gender = train$Gender
age = train$Age
ODJ = train$OutdoorJob
ODA = train$OutdoorActivities
smoking = train$SmokingHabit
pressure = train$Pressure
uv = train$UVIndex
# Continous Data
temp = train$Temperature
hum = train$Humidity
wind = train$WindSpeed
# Y value
ACT = train$ACTScore

# Test Set
# Catagorical Data
test_subject = test$UserNo.
test_loc = test$Location
test_gender = test$Gender
test_age = test$Age
test_ODJ = test$OutdoorJob
test_ODA = test$OutdoorActivities
test_smoking = test$SmokingHabit
test_pressure = test$Pressure
test_uv = test$UVIndex
# Continous Data
test_temp = test$Temperature
test_hum = test$Humidity
test_wind = test$WindSpeed
# Y value
test_ACT = test$ACTScore
```

## Finding outliers in the categorical/numeric data

To find outliers, we will have a box plot of each input that is
categorical and put it against the inputs that are continuous.Finding
the outliers using Boxplot (finds points outside 1.5\*IQR), as well as
to get a better understanding the distribution of the features a box
plot was made. We do this for each, train and test, set.

First plotting 1 categorical input at a time with temp humidity

```{r}
## NEED TO USE PACKAGE car for this to work because of the upper case B

# This box and whisker plot created the boxplot and also labels the points that are 
# 1.5 times outside the IQR, I then take those points, convert them from chr to
# numberic and store it as an outlier. I think take the union of all the outlires
# to keep track of when looking at other inputs that are continuous.

# Source for this: http://www.unige.ch/ses/sococ/cl/r/bapr.e.html

outlier_ageH = as.numeric(Boxplot(hum~age))
outlier_genderH = as.numeric(Boxplot(hum~gender))
outlier_smH = as.numeric(Boxplot(hum~smoking))
outlier_ODJH = as.numeric(Boxplot(hum~ODJ))
outlier_ODAH = as.numeric(Boxplot(hum~ODA))
outlier_pH = as.numeric(Boxplot(hum~pressure))
outlier_UVH = as.numeric(Boxplot(hum~uv))

# The last part combines all the outlier indices found as a vector and keeps all 
# number once, so say 114 is stored twice then the unique() will just take one 
# of them out
totOutlier_H = c(outlier_ageH, outlier_genderH, outlier_smH, outlier_ODJH, outlier_ODAH,outlier_pH, outlier_UVH)
table(totOutlier_H)
```

Next plotting 1 cat. at a time with

```{r}
## NEED TO USE PACKAGE car for this to work because of the upper case B

# This box and whisker plot created the boxplot and also labels the points that are 
# 1.5 times outside the IQR, I then take those points, convert them from chr to
# numberic and store it as an outlier. I think take the union of all the outliers
# to keep track of when looking at other inputs that are continuous.

outlier_ageT = as.numeric(Boxplot(temp~age))
outlier_genderT = as.numeric(Boxplot(temp~gender))
outlier_smT = as.numeric(Boxplot(temp~smoking))
outlier_ODJT = as.numeric(Boxplot(temp~ODJ))
outlier_ODAT = as.numeric(Boxplot(temp~ODA))
outlier_pT = as.numeric(Boxplot(temp~pressure))
outlier_UVT = as.numeric(Boxplot(temp~uv))

# The last part combines all the outlier indices found as a vector and keeps all 
# number once, so say 114 is stored twice then the unique() will just take one 
# of them out
totOutlier_T= c(outlier_ageT, outlier_genderT, outlier_smT, outlier_ODJT, outlier_ODAT,outlier_pT, outlier_UVT)

table(totOutlier_T)
```

lastly, plotting 1 cat at a time with windspeed

```{r}
# This box and whisker plot created the boxplot and also labels the points that are 
# 1.5 times outside the IQR, I then take those points, convert them from chr to
# numberic and store it as an outlier. I think take the union of all the outliers
# to keep track of when looking at other inputs that are continuous.

outlier_ageW = as.numeric(Boxplot(wind~age))
outlier_genderW = as.numeric(Boxplot(wind~gender))
outlier_smW = as.numeric(Boxplot(wind~smoking))
outlier_ODJW = as.numeric(Boxplot(wind~ODJ))
outlier_ODAW = as.numeric(Boxplot(wind~ODA))
outlier_pW = as.numeric(Boxplot(wind~pressure))
outlier_UVW = as.numeric(Boxplot(wind~uv))

# The last part combines all the outlier indices found as a vector and keeps all 
# number once, so say 114 is stored twice then the unique() will just take one 
# of them out

totOutlier_W = c(outlier_ageW, outlier_genderW, outlier_smW, outlier_ODJW, outlier_ODAW,outlier_pW, outlier_UVW)
table(totOutlier_W)


#Looking at the total amount of outliers now:


totOutliers = c(totOutlier_H, totOutlier_T, totOutlier_W)
#length(totOutliers)
#table(totOutliers)
#table(totOutliers) > 1
#which(table(totOutliers) > 1)

repOutliers = which(table(totOutliers) > 1) #Gives the indices for the the ones that show up more than once. 

# This ensures that if we run the code again it will not take more
# Data out, just noticed it was going down when this was not added
if(dim(train)[1] == 757){
  train= train[-c(repOutliers),]
}
# Visual help that outliers are only removed once
dim(asthmaData)
dim(test)
dim(train)

```

Check to see all inputs and categories are still represented

```{r}
# Age group above 50 lost 23 instances, not sig. for the group
# Males also lost 23 instances
# ODJ: Occationally lost 23 instances
# Extremely likely : lost 23 instances
# No smoking lost 23 instances
# Pressure 1007 lost 6 instances, 1008 lost 4,  1009 lost 2, 1010 lost 5
#            1011 lost 4, 1012 and 1013 lost 1 each (totalign 23 instances)
#UVindex: extreme lost 2 and the low lost 21.

## All in all, no group was lost or severly minimized during this process
table(asthmaData$Age) 
table(asthmaData$Gender)
table(asthmaData$OutdoorJob)
table(asthmaData$OutdoorActivities)
table(asthmaData$SmokingHabit)
table(asthmaData$Pressure)
table(asthmaData$UVIndex)
table(asthmaData$ACTScore)

table(train$UserNo.)
```

## Standardize Data set

```{r}
## standard normalize of continuous variable for mixture model.
train$Humidity = scale(train$Humidity, scale = TRUE)
train$Temperature = scale(train$Temperature, scale = TRUE)
train$WindSpeed = scale(train$WindSpeed, scale = TRUE)

test$Humidity = scale(test$Humidity, scale = TRUE)
test$Temperature = scale(test$Temperature, scale = TRUE)
test$WindSpeed = scale(test$WindSpeed, scale = TRUE)
```


## Bootstrapping

```{r}
# Ch. 11.1 - upsample 
set.seed(9560)
up_train <- upSample(x = train[, -ncol(train)],
                     y = train$UserNo.)                         
table(up_train$Class) 

```

## Mixted Effects Model

### Backward feature selection

```{r}

backmodel1 <- lmer(ACTScore ~ WindSpeed + Age + SmokingHabit + Humidity + Gender + OutdoorJob + OutdoorActivities + Pressure + UVIndex + (1|UserNo.) + (1|Location) + (0 + Gender|UserNo.) + (0 + Gender|Location) + (0 + Age|UserNo.) + (0 + Age|Location), data = up_train)

step_res1 <- step(backmodel1)
final1 <- get_model(step_res1)
anova(final1)
step_res1

```

```{r}
#backmodel2 <- lmer(ACT ~  wind + temp + hum + gender + age + ODJ + ODA + smoking +  pressure + uv + (1|subject) + (1|loc) + (0 + gender|subject) + (0 + gender|loc) + (0 + age|subject) + (0 + age|loc), data = train)

#step_res2 <- step(backmodel2)
#final2 <- get_model(step_res2)
#anova(final2)
#step_res2
```

```{r}
#backmodel3 <- lmer(ACT ~  wind + gender + ODJ + ODA + pressure + hum +  (1|subject) + (1|loc), data = train)

#step_res3 <- step(backmodel3)
#final3 <- get_model(step_res3)
#anova(final3)
#step_res3
```

```{r}
#backmodel4 <- lmer(ACT ~  wind + gender + ODJ  + pressure + hum +  (1|subject), data = train)

#step_res4 <- step(backmodel4)
#final4 <- get_model(step_res4)
#anova(final4)
#step_res4
```



## Testing lasso with model


```{r}
#set lambdas... go from 0 to 10^5, in 10 log steps
lambda <- seq(0.0001,0.5, length = 100)
BIC_vec <- rep(Inf, length(lambda))
AIC_vec <- rep(Inf, length(lambda))
Devianz_ma<-NULL
Coeff_ma<-NULL

# The for loop tests the different lambda values and records the BIC number.
# This is hypertuning the parameter lambda through
for (j in 1:10)
  {
    print(paste("Iteration ", j, sep=""))

glm1 =  try(glmmLasso(ACTScore~ WindSpeed + Temperature + as.factor(Gender) + as.factor(Pressure) + as.factor(UVIndex) + as.factor(Age)  + as.factor(OutdoorJob) +  Humidity,rnd = list(UserNo.=~1,Location=~ 1), lambda=lambda[j], data = up_train, switch.NR = TRUE,final.re = TRUE))

# Save BIC number 
if(class(glm1)!="try-error")
      {  
      BIC_vec[j]<-glm1$bic
      }
  }

```

```{r}

which.min(BIC_vec)
lambda[which.min(BIC_vec)]

```

```{r}
model_l = glmmLasso(ACTScore~ WindSpeed + Humidity + Temperature + as.factor(Gender) + as.factor(OutdoorJob) + as.factor(Pressure) + as.factor(Age) + as.factor(UVIndex),rnd = list(UserNo.=~1 ,Location=~ 1), lambda = 0.01524848, data = train,switch.NR = TRUE,final.re = TRUE)
# https://www.aggieerin.com/post/lasso-myself-numbers/
#0.4132323

summary(model_l)
```



```{r}
#model_l$bic
#model_l$aic
model_l$ranef

```

It has been confirmed, by backwards feature selection and Lasso that the random effects terms are not significant. This can be seen from the fact that backwards feature method did not select the random effects and the lasso method found the coefficients to be nearly zero.

So we try fitting with a linear model.




```{r}
library(arm)
# Chapter 4
set.seed(23)

folds <- groupKFold(up_train$UserNo., k = 7) 

# Chapter 5
train_control <- trainControl(method = "LOOCV", index = folds)

model1 <- train(ACTScore ~ Gender + WindSpeed + Temperature + Age + Humidity + UVIndex + OutdoorJob + OutdoorActivities + Pressure, data = up_train[,-ncol(up_train)], method = "lm", trControl = train_control)

model2 <- train(ACTScore ~ WindSpeed + Humidity + Pressure + UVIndex, data = up_train[,-ncol(up_train)], method = "lm", trControl = train_control)

#model3 <- train(ACTScore ~ Gender + WindSpeed + Temperature + Age + Humidity + UVIndex + OutdoorJob + OutdoorActivities + Pressure, data = up_train[,-ncol(up_train)], method = "'bayesglm'", trControl = train_control) # Package: arm
# Bayesian regression, idea from: https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-021-01704-6
```


```{r}
#model1$results
#model1$finalModel

#model2$results
#model2$finalModel
summary(model1)
summary(model2)
```

```{r}


rsq <- function(formula, data, indices) {
  d <- data[indices,]
  fit <- lm(formula, data=d)
  return(summary(fit)$r.squared)
}

rs <- vector()
j <- 1

#for (i in 1:7) {
#  results <- boot(data = train, statistic = rsq, R = 400, formula = ACTScore ~ WindSpeed + Humidity + as.factor(Gender) + as.factor(OutdoorJob) + #as.factor(OutdoorActivities) + as.factor(Pressure) + as.factor(UVIndex))
  
# print(i)
  #rs[j] <- results$t0
  #j <- j+1
#}

#rs
```


```{r}
plot(results)
```



# Predicting on validation set
```{r}
#pred_l = predict(model_l, validation, interval="predict") 
#error_l = round(pred_l) - validation$ACTScore
#error_l

#pred_sb = predict(model_sb, validation, interval='predict')
#error_sb =  round(pred_sb[,1]) - validation$ACTScore
#error_sb

#data.frame(R2 = R2(pred_l, validation $ ACTScore),
#           RMSE = RMSE(pred_l, validation $ ACTScore), 
#           MAE = MAE(pred_l, validation $ ACTScore))

#data.frame(R2 = R2(pred_sb, validation $ ACTScore), 
#           RMSE = RMSE(pred_sb, validation $ ACTScore), 
#           MAE = MAE(pred_sb, validation $ ACTScore))
```


# Generaliation to test set
```{r}

BestModel = predict(model_l, test, interval="predict") 
error_l = round(BestModel) - test$ACTScore

data.frame(R2 = R2(BestModel, test $ ACTScore),
           RMSE = RMSE(BestModel, test $ ACTScore), 
           MAE = MAE(BestModel, test $ ACTScore))
```













