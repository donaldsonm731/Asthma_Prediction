---
title: "Asthma Attack Predictions"
author: "Kadie Iverson & Matthew Donaldson"
date: "2/8/2022"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

---
title: "Asthma Attack Predictions"
author: "Kadie Iverson & Matthew Donaldson"
date: "2/8/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(car)
library(olsrr)
library(lme4)
library(shiny)
library(cvms)
library(groupdata2)
library(lmerTest)
library(glmnet)
library(glmmLasso)
library(boot)
library(arm)
```

Sources: 
Mixture model:
<https://lme4.r-forge.r-project.org/slides/2011-03-16-Amsterdam/2Longitudinal.pdf> 
Catet Package:
<https://topepo.github.io/caret/feature-selection-overview.html>
Vraible selection:
<https://cran.r-project.org/web/packages/olsrr/vignettes/variable_selection.html> 
Boxplots: <http://www.unige.ch/ses/sococ/cl/r/bapr.e.html> 
Regressing Cat. data:
<https://advstats.psychstat.org/book/mregression/catpredictor.php> 
Coding Cat. Data:
<https://stats.oarc.ucla.edu/r/modules/coding-for-categorical-variables-in-regression-models/>

# Practicum of Data Analytics Project

## Importing

```{r}
################################################################################
# Importing the data set 
asthmaDataOriginal= read.csv("Asthma_Data_File.csv")  

# Removing id number for each subject and copying original file to not corrupt it
asthmaData = data.frame(asthmaDataOriginal[,-2])  

# Looking at data
head(asthmaData)  #Looking at the first 6 entries
dim(asthmaData)
```

## Data Description

The first summary doesn't give us to much information because half the
inputs are chr. So, before looking at the properties of each feature we
need to factor the data.

```{r}
# Summary gives a description for each input and output, giving you a sense of what the data looks like
summary(asthmaData)

```

Factoring is done in two stages. One for variables were there is an order and 
one for no order. For no order the lapply() function is used and for when we want
to specify order the factor() function with order = TRUE is used. The we check using
the str() function.

```{r}
# Only want  to factor the categories.
# Ordered categorical variables: Age, OutDoorJob, OutDoorActivities, UVIndex
# Else: just categorical variables to factor.
# Continuous Varaibale: temp, humidity, windspeed and pressure
# Dependent variable: ACTScore
names = names(asthmaData[,c(1,2, 4,7)]) 
asthmaData[,names] = lapply(asthmaData[,names], factor)

asthmaData[,3] = factor(asthmaData[,3], order = TRUE, levels = c("19-30", "31-40", "41-50", "Above 50"))
asthmaData[,5] = factor(asthmaData[,5], order = TRUE, levels = c("Rarely", "Occasionally", "Frequently"))
asthmaData[,6] = factor(asthmaData[,6], order = TRUE, levels = c("Not at all likely", "Neither likely or dislikely", "Extremely likely"))
asthmaData[,11] = factor(asthmaData[,11], order = TRUE, levels = c("Low", "Extreme"))

str(asthmaData)

```

Next we aim to see the distribution for each variable and subjects by bar plots
and histograms. As can be seen from the plots below, we have unbalanced instances
for subjects, outdoor activities and jobs...


```{r}
barplot(prop.table(table(asthmaData$UserNo.)), main = 'Subject')
barplot(prop.table(table(asthmaData$Age)), main = 'Age')
barplot(prop.table(table(asthmaData$Gender)), main = 'Gender')
barplot(prop.table(table(asthmaData$OutdoorJob)), main = 'Out Door Job')
barplot(prop.table(table(asthmaData$OutdoorActivities)), main = 'Out Door Activities')
barplot(prop.table(table(asthmaData$SmokingHabit)), main = 'Smoking Habit')
barplot(prop.table(table(asthmaData$UVIndex)), main = 'UV Index')

hist(asthmaData$Humidity, breaks = 25, main = 'Humidity')
hist(asthmaData$Temperature, breaks = 25, main = 'temp')
hist(asthmaData$WindSpeed, breaks = 25, main = 'wind')
hist(asthmaData$Pressure, breaks = 12, main = 'Pressure')
hist(asthmaData$ACTScore,breaks = 17, main = 'ACT score')
```

This is just to get the actual values for each level in the groups based on the 
bar and histograms above.
```{r}
table(asthmaData$Age) 
table(asthmaData$Gender)
table(asthmaData$OutdoorJob)
table(asthmaData$OutdoorActivities)
table(asthmaData$SmokingHabit)
table(asthmaData$UVIndex)
table(asthmaData$ACTScore)
################################################################################
```




## Splitting into Train & Test

Before anything else, we split the data into a training and test set. This ensures
that we do not have any data leakage when we standardize our training set and so
the test set is left alone.
```{r}
################################################################################
set.seed(23)

library(dplyr)
training <- sample(unique(asthmaData$UserNo.), 7, replace = FALSE)

train <- asthmaData %>% filter(asthmaData$UserNo. %in% training)
test <-  asthmaData %>% filter(!asthmaData$UserNo. %in% training)

# This is only used to ensure that once we remove the outliers we done continue
# to remove more indices if we run more than once. (Forshadowing to fix a problem)
train_original = train

# Looking at dimensions of training and test set
dim(train)
dim(test)
summary(train)
################################################################################
```


## Outlier Detection



To find outliers, we will have a box plot of each input that is
categorical and put it against the inputs that are continuous.Finding
the outliers using Boxplot (finds points outside 1.5\*IQR), as well as
to get a better understanding the distribution of the features a box
plot was made. We do this for the training set.

First plotting humidity against the categorical variables

```{r}
###############################################################################
## NEED TO USE PACKAGE car for this to work because of the upper case B

# This box and whisker plot created the boxplot and also labels the points that are 
# 1.5 times outside the IQR, I then take those points, convert them from chr to
# numberic and store it as an outlier. I think take the union of all the outlires
# to keep track of when looking at other inputs that are continuous.

# Source for this: http://www.unige.ch/ses/sococ/cl/r/bapr.e.html

outlier_ageH = as.numeric(Boxplot(train$Humidity~train$Age))
outlier_genderH = as.numeric(Boxplot(train$Humidity~train$Gender))
outlier_smH = as.numeric(Boxplot(train$Humidity~train$SmokingHabit))
outlier_odjH = as.numeric(Boxplot(train$Humidity~train$OutdoorJob))
outlier_odaH = as.numeric(Boxplot(train$Humidity~train$OutdoorActivities))
outlier_uvH = as.numeric(Boxplot(train$Humidity~train$UVIndex))

# The last part combines all the outlier indices found as a vector
totOutlier_H = c(outlier_ageH, outlier_genderH, outlier_smH, outlier_odjH, outlier_odaH, outlier_uvH)
table(totOutlier_H) # Potential outlier indecies
```

Next plotting temp. against cat.

```{r}
## NEED TO USE PACKAGE car for this to work because of the upper case B

# This box and whisker plot created the boxplot and also labels the points that are 
# 1.5 times outside the IQR, I then take those points, convert them from chr to
# numberic and store it as an outlier. I think take the union of all the outliers
# to keep track of when looking at other inputs that are continuous.

outlier_ageT = as.numeric(Boxplot(train$Temperature~train$Age))
outlier_genderT = as.numeric(Boxplot(train$Temperature~train$Gender))
outlier_smT = as.numeric(Boxplot(train$Temperature~train$SmokingHabit))
outlier_odjT = as.numeric(Boxplot(train$Temperature~train$OutdoorJob))
outlier_odaT = as.numeric(Boxplot(train$Temperature~train$OutdoorActivities))
outlier_pT = as.numeric(Boxplot(train$Temperature~train$Pressure))
outlier_uvT = as.numeric(Boxplot(train$Temperature~train$UVIndex))

# The last part combines all the outlier indices found as a vector and keeps all 
# number once, so say 114 is stored twice then the unique() will just take one 
# of them out
totOutlier_T= c(outlier_ageT, outlier_genderT, outlier_smT, outlier_odjT, outlier_odaT,outlier_pT, outlier_uvT)

table(totOutlier_T)
```

plotting windspeed with cat.

```{r}
# This box and whisker plot created the boxplot and also labels the points that are 
# 1.5 times outside the IQR, I then take those points, convert them from chr to
# numberic and store it as an outlier. I think take the union of all the outliers
# to keep track of when looking at other inputs that are continuous.

outlier_ageW = as.numeric(Boxplot(train$WindSpeed~train$Age))
outlier_genderW = as.numeric(Boxplot(train$WindSpeed~train$Gender))
outlier_smW = as.numeric(Boxplot(train$WindSpeed~train$SmokingHabit))
outlier_odjW = as.numeric(Boxplot(train$WindSpeed~train$OutdoorJob))
outlier_odaW = as.numeric(Boxplot(train$WindSpeed~train$OutdoorActivities))
outlier_uvW = as.numeric(Boxplot(train$WindSpeed~train$UVIndex))

# The last part combines all the outlier indices found as a vector and keeps all 
# number once, so say 114 is stored twice then the unique() will just take one 
# of them out

totOutlier_W = c(outlier_ageW, outlier_genderW, outlier_smW, outlier_odjW, outlier_odaW, outlier_uvW)
table(totOutlier_W)

```

Plotting pressure with cat.
```{r}
# This box and whisker plot created the boxplot and also labels the points that are 
# 1.5 times outside the IQR, I then take those points, convert them from chr to
# numberic and store it as an outlier. I think take the union of all the outliers
# to keep track of when looking at other inputs that are continuous.

outlier_ageP = as.numeric(Boxplot(train$Pressure~train$Age))
outlier_genderP = as.numeric(Boxplot(train$Pressure~train$Gender))
outlier_smP = as.numeric(Boxplot(train$Pressure~train$SmokingHabit))
outlier_odjP = as.numeric(Boxplot(train$Pressure~train$OutdoorJob))
outlier_odaP = as.numeric(Boxplot(train$Pressure~train$OutdoorActivities))
outlier_uvP = as.numeric(Boxplot(train$Pressure~train$UVIndex))

# The last part combines all the outlier indices found as a vector and keeps all 
# number once, so say 114 is stored twice then the unique() will just take one 
# of them out

totOutlier_P = c(outlier_ageP, outlier_genderP, outlier_smP, outlier_odjP, outlier_uvP)
table(totOutlier_P)

```


Now combining all indices found to be potential outliers and taking any indices 
that was found more than once.
```{r}
# Combines all outliers into on vector.
totOutliers = c(totOutlier_H, totOutlier_T, totOutlier_W, totOutlier_P)

#Gives the indices for the the ones that show up more than once. 
repOutliers = which(table(totOutliers) > 1) 

# This ensures that if we run the code again it will not take more
# Data out, just noticed it was going down when this was not added

# Here we use the train_original from when we made the test set to compare if 
# outliers have already been taken out.
if(dim(train)[1] == dim(train_original)[1]){
  train= train[-c(repOutliers),]
}
# Visual help that outliers are only removed once
dim(asthmaData)
dim(train)
length(repOutliers)
################################################################################
```


## Standardize Data set

```{r}
################################################################################
## standard normalize of continuous variable for models.
train$Humidity = scale(train$Humidity, scale = TRUE)
train$Temperature = scale(train$Temperature, scale = TRUE)
train$WindSpeed = scale(train$WindSpeed, scale = TRUE)
train$Pressure = scale(train$Pressure, scale = TRUE)

test$Humidity = scale(test$Humidity, scale = TRUE)
test$Temperature = scale(test$Temperature, scale = TRUE)
test$WindSpeed = scale(test$WindSpeed, scale = TRUE)
test$Pressure = scale(test$Pressure, scale = TRUE)
################################################################################
```


## Understanding Correlation b/w variables through VIF number

We try and understand the correlation of a variable through the VIF number.
VIF provides an index that measure show much the variance of an estimated 
regression coefficient is increased because of linearity. If VIF is above
5 or 10 then it is an indication that the associated regression coefficients are 
poorly estimated due to multicollinearity.
```{r}
################################################################################
library(faraway)
model.vif = lm(ACTScore ~ Temperature + WindSpeed + Age + SmokingHabit + Humidity + Gender + OutdoorJob + OutdoorActivities + Pressure + UVIndex, data = train)

vif(model.vif)

################################################################################
```




## Feature Selection for mixed effects model

We first perform backwards feature selection with our mixed effects model. In the
step() function from lmerTest package, the function first looks at the significance of
random effects then looks at the fixed effects. Picks values to drop based on 
p-values. 
```{r}
################################################################################

# https://www.rdocumentation.org/packages/lmerTest/versions/2.0-36/topics/step

model.lmer.step <- lmerTest::lmer(ACTScore ~ Temperature + WindSpeed + Age + SmokingHabit + Humidity + Gender + OutdoorJob + OutdoorActivities + Pressure + UVIndex+ (1|UserNo.) + (1|Location) + (0 + Gender|UserNo.) + (0 + Gender|Location) + (0 + Age|UserNo.) + (0 + Age|Location), data = train)

step_res.lmer <- lmerTest::step(model.lmer.step)
model.lmer <- get_model(step_res.lmer)
anova(model.lmer)
step_res.lmer
```
From the output above we can see the model found is:
ACTScore ~ WindSpeed + Humidity + Gender + OutdoorJob + Pressure + UVIndex

From this first we notice that none of the random effects are used. 
And then also 6 variables are left.


## Feature Selection for fixed linear model

Next we do feature selection on a fixed linear model. This function chooses the 
features to from by AIC (), but other wise same process as step() function above.
```{r}
model.lm <- lm(ACTScore ~ Temperature + WindSpeed + Age + SmokingHabit + Humidity + Gender + OutdoorJob + OutdoorActivities + Pressure + UVIndex, data = train)

step_res.lm <- step(model.lm)

###############################################################################
```

Model found from above:
ACTScore ~ WindSpeed + Humidity + Gender + OutdoorJob + Pressure + UVIndex

```{r}
model.lm = lm(ACTScore ~ WindSpeed + Humidity + Gender + OutdoorJob + Pressure + UVIndex, data = train)
summary(model.lm)
summary(model.lm)$sigma
```


## C.I. for coefficients for lm
```{r}
model.lm$coefficients
confint(model.lm)
```

## Lasso with linear model

```{r}
x = data.matrix(train[,c('Humidity', 'Temperature', 'Pressure', 'Age', 'Gender', 'UVIndex', 'OutdoorJob', 'OutdoorActivities', 'SmokingHabit', 'WindSpeed')])
y = train[,'ACTScore']

model.glmnet.cv = cv.glmnet(x,y, alpha = 1)
best_lambda = model.glmnet.cv$lambda.min
#plot(model.glmnet)

model.glmnet = glmnet(x,y, alpha = 1,lambda = best_lambda)
coef(model.glmnet)
best_lambda

#confint(model.glmnet)
###############################################################################
```

## Picking a model through Leave one out CV (by subject)

### LOOCV for lm
```{r}
library(rsample)
set.seed(23)


splits <- group_vfold_cv(train, group = "UserNo.", v = 7)

Caret_splits <- rsample2caret(splits)
tc_grouped <- trainControl(method = "cv", number = 7, 
                           index = Caret_splits$index,
                           indexOut = Caret_splits$indexOut)

model.lm.loocv <- train(ACTScore ~ WindSpeed + Humidity + Gender + OutdoorJob + Pressure + UVIndex, 
                   data = train, method = "lm", family = "gaussian",
                   trControl = tc_grouped)

model.lm.loocv$results

```

### LOOCV for glmnet
```{r}
################################################################################
library(rsample)
library(Matrix)
set.seed(23)


splits <- group_vfold_cv(train, group = "UserNo.", v = 7)

Caret_splits <- rsample2caret(splits)

tunegrid = expand.grid(alpha = 0:1, lambda=seq(0.0001, 1, length = 20))



model.glmnet.loocv<- train(ACTScore ~ Temperature + WindSpeed + Age + SmokingHabit + Humidity + Gender + OutdoorJob + OutdoorActivities + Pressure + UVIndex , data = train, 
                   method = "glmnet", family = "gaussian", tuneGrid = tunegrid,
                   trControl = tc_grouped)

model.glmnet.loocv$results
which(model.glmnet.loocv$results$lambda == model.glmnet.loocv$bestTune$lambda)[2]
```


## Comparing results
```{r}
print(paste0("LM Model RMSE: ", round(model.lm.loocv$results$RMSE, digits = 3)))
print(paste0("Lasso Model RMSE: ", round(model.glmnet.loocv$results$RMSE[which(model.glmnet.loocv$results$lambda == model.glmnet.loocv$bestTune$lambda)[2]], digits = 3)))


print(paste0("LM Model R-Squared: ", round(model.lm.loocv$results$Rsquared, digits = 4)))
print(paste0("Lasso Model R-Squared: ", round(model.glmnet.loocv$results$Rsquared[which(model.glmnet.loocv$results$lambda == model.glmnet.loocv$bestTune$lambda)[2]], digits = 4)))

print(paste0("LM Model MAE: ", round(model.lm.loocv$results$MAE, digits = 3)))
print(paste0("Lasso Model MAE: ", round(model.glmnet.loocv$results$MAE[which(model.glmnet.loocv$results$lambda == model.glmnet.loocv$bestTune$lambda)[2]], digits = 3)))

```

The list above shows the MSE, $R^2$ and MAE for LM and Lasso. From this we feel that
the LM model is a better fit because it has a higher $R^2$ and lowest 
MSE and MAE.

## Predict on Test Set
```{r}
###############################################################################
library(caret)

y_pred = predict(model.lm, newdata = test, interval = "prediction")
y_pred
data.frame(R2 = R2(y_pred, test $ ACTScore),
           RMSE = RMSE(y_pred, test $ ACTScore), 
           MAE = MAE(y_pred, test $ ACTScore))

model.lm$coefficients

```

## Model Verification
```{r}
#plot(model.lm)

y_pred = predict(model.lm, newdata = test, interval = "confidence")
y_pred
```



```{r}
summary(model.lm)
```




